{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Contest3(NLP).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AvMpINC-7tS"
      },
      "source": [
        "## ***SET UP AND PREPROCESS***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43_xqDiS_DUb"
      },
      "source": [
        "INSTALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB2p-xvVk2wi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7100b5f3-c803-4196-9992-c2b6d578ad93"
      },
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkp8YzvmcG1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fa98aa-35da-4f09-eeb2-990edd62dfa7"
      },
      "source": [
        "!pip install pythainlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pythainlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/e7/837dd9ab52fac889af830dc094a1598251f70004a2f1707ab0ff8dc0f63a/pythainlp-2.3.1-py3-none-any.whl (11.0MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0MB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (0.9.7)\n",
            "Collecting tinydb>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/af/cd/1ce3d93818cdeda0446b8033d21e5f32daeb3a866bbafd878a9a62058a9c/tinydb-4.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2020.12.5)\n",
            "Installing collected packages: tinydb, pythainlp\n",
            "Successfully installed pythainlp-2.3.1 tinydb-4.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPXyX543zF3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a803b1-abe8-4ebb-f073-4102602a28a0"
      },
      "source": [
        "import json\n",
        "from pythainlp.tag import pos_tag\n",
        "from google.colab import drive\n",
        "import sklearn_crfsuite\n",
        "import sklearn_crfsuite.metrics\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC5nD54F_E40"
      },
      "source": [
        "LOAD DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tllj_Zfne3m"
      },
      "source": [
        "อย่ายุ่งกับส่วนนี้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRpoJxCkndhp"
      },
      "source": [
        "def load_data(file_path):\n",
        "    data = open(file_path).read()\n",
        "    list_all = data.split('\\n')\n",
        "    sentence_in_list_all = []\n",
        "    sentence = []\n",
        "    answer_isus = [] \n",
        "    Y = []\n",
        "    X = []\n",
        "    # print(list_all)\n",
        "    for i in list_all:\n",
        "      if i == '':\n",
        "        sentence_in_list_all.append(sentence)\n",
        "        sentence = []\n",
        "      else:\n",
        "        sentence.append(i)\n",
        "    for sentences in sentence_in_list_all:\n",
        "      answer = [] \n",
        "      keep_all_word_and_ner_each_sentence = []\n",
        "      for len_word in range(len(sentences)):\n",
        "        word_and_ner = []\n",
        "        word_and_ner_split = sentences[len_word].split(\"\\t\")\n",
        "        word_and_ner.append(word_and_ner_split[1][2:])\n",
        "        word_and_ner.append(word_and_ner_split[0])\n",
        "        keep_all_word_and_ner_each_sentence.append(word_and_ner)\n",
        "      # print(keep_all_word_and_ner_each_sentence)\n",
        "\n",
        "      replaced = ['x' if x[0]=='' else x for x in keep_all_word_and_ner_each_sentence]\n",
        "      # print(replaced)\n",
        "      keep = []\n",
        "      for len_word_and_ner in range(len(replaced)):\n",
        "        if replaced[len_word_and_ner] != 'x':\n",
        "          if len_word_and_ner == len(replaced)-1:\n",
        "            list_answer = []\n",
        "            # print(\"1: add\",replaced[len_word_and_ner][1])\n",
        "            keep.append(replaced[len_word_and_ner][1])\n",
        "            list_answer.append(replaced[len_word_and_ner][0])\n",
        "            list_answer.append(''.join(keep))\n",
        "            answer.append(tuple(list_answer))\n",
        "            # print('KEEP:',keep)\n",
        "          elif replaced[len_word_and_ner][0] == replaced[len_word_and_ner+1][0]:\n",
        "            # print(\"2: add\",replaced[len_word_and_ner][1])\n",
        "            keep.append(replaced[len_word_and_ner][1])\n",
        "            # print('KEEP:',keep)\n",
        "          elif replaced[len_word_and_ner][0] != replaced[len_word_and_ner+1][0]:\n",
        "            list_answer = []\n",
        "            # print(\"3: add\",replaced[len_word_and_ner][1])\n",
        "            keep.append(replaced[len_word_and_ner][1])\n",
        "            # print('KEEP:',keep)\n",
        "            list_answer.append(replaced[len_word_and_ner][0])\n",
        "            list_answer.append(''.join(keep))\n",
        "            answer.append(tuple(list_answer))\n",
        "            keep = []\n",
        "        else:\n",
        "          # print(4)\n",
        "          continue\n",
        "      # print(answer)\n",
        "      answer_isus.append(answer) #เอาไป evaluate \n",
        "      list_tempX = []\n",
        "      list_tempY = []\n",
        "      for i in answer:\n",
        "        list_tempX.append(i[1])\n",
        "        list_tempY.append(i[0])\n",
        "      X.append(list_tempX)\n",
        "      # X.append(pos_tag(list_tempX, corpus='orchid_ud'))\n",
        "      Y.append(list_tempY)\n",
        "    return (X,Y,answer_isus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeqV8VMWnggW"
      },
      "source": [
        "ทดลอง B I I I มาต่อกัน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9XkgtFP9oKC"
      },
      "source": [
        "def load_data(file_path):\n",
        "    data = open(file_path).read()\n",
        "    list_all = data.split('\\n')\n",
        "    sentence_in_list_all = []\n",
        "    sentence = []\n",
        "    answer_isus = [] \n",
        "    Y = []\n",
        "    X = []\n",
        "    # print(list_all)\n",
        "    for i in list_all:\n",
        "      if i == '':\n",
        "        sentence_in_list_all.append(sentence)\n",
        "        sentence = []\n",
        "      else:\n",
        "        sentence.append(i)\n",
        "    for sentences in sentence_in_list_all:\n",
        "      answer = [] \n",
        "      keep_all_word_and_ner_each_sentence = []\n",
        "      for len_word in range(len(sentences)):\n",
        "        word_and_ner = []\n",
        "        word_and_ner_split = sentences[len_word].split(\"\\t\")\n",
        "        word_and_ner.append(word_and_ner_split[1][:])\n",
        "        word_and_ner.append(word_and_ner_split[0])\n",
        "        keep_all_word_and_ner_each_sentence.append(word_and_ner)\n",
        "      # print(keep_all_word_and_ner_each_sentence)\n",
        "\n",
        "      replaced = ['x' if x[0]=='O' else x for x in keep_all_word_and_ner_each_sentence]\n",
        "      # print(replaced)\n",
        "      keep = []\n",
        "      for len_word_and_ner in range(len(replaced)):\n",
        "        if replaced[len_word_and_ner] != 'x':\n",
        "          if len_word_and_ner == len(replaced)-1:\n",
        "            list_answer = []\n",
        "            # print(\"1: add\",replaced[len_word_and_ner][1])\n",
        "            keep.append(replaced[len_word_and_ner][1])\n",
        "            list_answer.append(replaced[len_word_and_ner][0])\n",
        "            list_answer.append(''.join(keep))\n",
        "            answer.append(tuple(list_answer))\n",
        "            # print('KEEP:',keep)\n",
        "\n",
        "\n",
        "          # [['B_PER', 'ทดสอบ'],['B_PER', 'ธรรมนูญ'], ['I_PER', ' '], ['I_PER', 'ศรี'], ['I_PER', 'โรจน์'], 'x', 'x', 'x', 'x', ['B_NUM', '4']]\n",
        "\n",
        "          elif (replaced[len_word_and_ner][0][2:] == replaced[len_word_and_ner+1][0][2:]): #ตรวจว่า PER == PER \n",
        "            if replaced[len_word_and_ner][0][0] == 'B' and replaced[len_word_and_ner+1][0][0] == 'I': #ตรวจว่าถ้าตอนนี้เป็น B และตัวถัดไปเป็น I ต้องเอามันมาติดกัน \n",
        "              keep.append(replaced[len_word_and_ner][1]) #เก็บคำของ B ไว้ก่อน\n",
        "            elif (replaced[len_word_and_ner][0][0] == 'I' and replaced[len_word_and_ner+1][0][0] == 'I'):\n",
        "              keep.append(replaced[len_word_and_ner][1])\n",
        "\n",
        "            elif (replaced[len_word_and_ner][0][0] == 'I' and replaced[len_word_and_ner+1][0][0] == 'B'):\n",
        "              list_answer = []\n",
        "              keep.append(replaced[len_word_and_ner][1])\n",
        "              list_answer.append(replaced[len_word_and_ner][0][2:])\n",
        "              list_answer.append(''.join(keep))\n",
        "              answer.append(tuple(list_answer))\n",
        "              keep = []\n",
        "            elif (replaced[len_word_and_ner][0][0] == 'B' and replaced[len_word_and_ner+1][0][0] == 'B'): #กรณีเป็นอย่างอื่นคืิอเราต้องแยกกัน\n",
        "              keep = []\n",
        "              list_answer = []\n",
        "              keep.append(replaced[len_word_and_ner][1])\n",
        "              list_answer.append(replaced[len_word_and_ner][0][2:])\n",
        "              list_answer.append(''.join(keep))\n",
        "              answer.append(tuple(list_answer))\n",
        "              keep = []\n",
        "          elif (replaced[len_word_and_ner][0][2:] != replaced[len_word_and_ner+1][0][2:]):\n",
        "            list_answer = []\n",
        "            keep.append(replaced[len_word_and_ner][1])\n",
        "            list_answer.append(replaced[len_word_and_ner][0][2:])\n",
        "            list_answer.append(''.join(keep))\n",
        "            answer.append(tuple(list_answer))\n",
        "            keep = []\n",
        "        else:\n",
        "          # print(4)\n",
        "          continue\n",
        "      # print(answer)\n",
        "      answer_isus.append(answer) #เอาไป evaluate \n",
        "      list_tempX = []\n",
        "      list_tempY = []\n",
        "      for i in answer:\n",
        "        list_tempX.append(i[1])\n",
        "        list_tempY.append(i[0])\n",
        "      # X.append(list_tempX)\n",
        "      X.append(pos_tag(list_tempX, corpus='orchid_ud'))\n",
        "      Y.append(list_tempY)\n",
        "    return (X,Y,answer_isus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR56cIOnp6S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799bad1b-b06b-4582-bad4-8a4463e34927"
      },
      "source": [
        "# load_data('earth.tsv')\n",
        "# (earth1,earth2,answer) = \n",
        "\n",
        "# json.dump(answer, open('predicted_train_entities.json', encoding='utf8', mode='w'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['B_PER', 'ธรรมนูญ'], 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x']\n",
            "[('PER', 'ธรรมนูญ')]\n",
            "[['B_PER', 'ธรรมนูญ'], ['I_PER', ' '], ['I_PER', 'ศรี'], ['I_PER', 'โรจน์'], 'x', 'x', 'x', 'x', ['B_NUM', '4'], 'x', 'x', 'x', 'x', ['B_NUM', '68'], 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', ['B_MEA', '19'], ['I_MEA', ' '], ['I_MEA', 'ปี'], 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', ['B_NUM', '18'], 'x', 'x', 'x', 'x', 'x']\n",
            "[('PER', 'ธรรมนูญ ศรีโรจน์'), ('NUM', '4'), ('NUM', '68'), ('MEA', '19 ปี'), ('NUM', '18')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX4P__9wnSTq",
        "outputId": "0efcd09f-bc65-4bf6-e811-091ae7d444f3"
      },
      "source": [
        "earth1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('ธรรมนูญ', 'NOUN')],\n",
              " [('ธรรมนูญ ศรีโรจน์', 'NOUN'),\n",
              "  ('4', 'NUM'),\n",
              "  ('68', 'NUM'),\n",
              "  ('19 ปี', 'NUM'),\n",
              "  ('18', 'NUM')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVtOMR0CL8k"
      },
      "source": [
        "Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEqsJTa1CuJ0"
      },
      "source": [
        "def pos_features(i, tokens):\n",
        "    if i==0:\n",
        "      return {'w0 pos': tokens[i][1]}\n",
        "    if i==1:\n",
        "      return {'w0 pos': tokens[i][1],'w-1 pos': tokens[i-1][1]}\n",
        "    if i==len(tokens)-1:\n",
        "      return {'w0 pos': tokens[i][1],'w-1 pos': tokens[i-1][1]}\n",
        "    else:\n",
        "      return {'w0 pos': tokens[i][1],'w-1 pos': tokens[i-1][1], 'w+1 pos': tokens[i+1][1]}\n",
        "    \n",
        "def pos_conjunctive_features(i, tokens):\n",
        "  def conjunctive_pos(i, tokens, positions):\n",
        "      position_str = ' '.join([str(x) for x in positions])\n",
        "      words = []\n",
        "      for position in positions:\n",
        "          if 0 <= i + position < len(tokens): \n",
        "              words.append(tokens[i+position][1])\n",
        "      return {'word i and word '+ position_str: tokens[i][1] + '_' + '_'.join(words)}\n",
        "  return conjunctive_pos(i, tokens, [-1, 1])   \n",
        "\n",
        "def word_features(i, tokens):\n",
        "    if i==0:\n",
        "      return {'w0 word': tokens[i][0]}\n",
        "    if i==1:\n",
        "      return {'w0 word': tokens[i][0],'w-1 word': tokens[i-1][0]}\n",
        "    if i==len(tokens)-1:\n",
        "      return {'w0 pos': tokens[i][0],'w-1 pos': tokens[i-1][0]}        \n",
        "    else:    \n",
        "      return {'w0 pos': tokens[i][0],'w-1 pos': tokens[i-1][0], 'w+1 pos': tokens[i+1][0]}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUviBUhm_NPI"
      },
      "source": [
        "# ***TRAINING MODEL***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAhMBQpf_zsV"
      },
      "source": [
        "**เตรียมข้อมูล**\n",
        "\n",
        "กรณีทำแค่ Train และ Dev\n",
        "จะสามารถส่งข้อมูลเข้า load_data แล้วเทรนโมเดลได้เลยทันที"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbmAdPoghBhG"
      },
      "source": [
        "X_train, Y_train,evaluate_train = load_data('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/train_auto_tok.tsv')\n",
        "X_dev, Y_dev,evaluate_dev = load_data('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/dev_auto_tok.tsv')\n",
        "# X_test, Y_test,evaluate_test= load_data('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/dev_auto_tok.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ01f4Qkjp4f",
        "outputId": "d076752c-2d90-494d-8945-10ef3bb3716c"
      },
      "source": [
        "X_train.pop()\n",
        "Y_train.pop()\n",
        "X_dev.pop()\n",
        "Y_dev.pop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAZZw2m14wpq"
      },
      "source": [
        "def Union(lst1, lst2):\n",
        "    final_list = lst1 + lst2\n",
        "    return final_list\n",
        "\n",
        "X_all = Union(X_train, X_dev)\n",
        "Y_all = Union(Y_train, Y_dev)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCg3Esq85UQw",
        "outputId": "2167d801-3374-4bb1-b307-edfaede56cdd"
      },
      "source": [
        "len(X_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68930"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVAwK_q5XSK",
        "outputId": "5737557e-87a6-441d-ad61-5d18b99a6366"
      },
      "source": [
        "len(Y_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68930"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYbjUYcavkgp"
      },
      "source": [
        "json.dump(evaluate_train, open('predicted_train_entities.json', encoding='utf8', mode='w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwKZ8g18jVI5",
        "outputId": "49e66ca8-26a7-4621-ad54-eda7cedeb436"
      },
      "source": [
        "X_train[-5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('เวลาประมาณ 09.20 น.', 'NOUN'),\n",
              " ('นาย', 'NOUN'),\n",
              " ('รองพล เจริญพันธ์', 'PROPN'),\n",
              " ('ทำเนียบรัฐบาล', 'NOUN'),\n",
              " ('รัฐมนตรี', 'NOUN'),\n",
              " ('สำนักนายกรัฐมนตรี', 'NOUN'),\n",
              " ('นาย', 'NOUN'),\n",
              " ('เนวิน ชิดชอบ', 'PROPN'),\n",
              " ('สำนักนายกรัฐมนตรี', 'PROPN'),\n",
              " ('คณะปฏิรูปฯ', 'NOUN'),\n",
              " ('กองบัญชาการ', 'NOUN'),\n",
              " ('กองทัพบก', 'NOUN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TES8-7BClB4r",
        "outputId": "bcae0a33-6792-4c85-cb20-3a084bb8fd8b"
      },
      "source": [
        "pip install attacut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting attacut\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/56/4ab7204bde7468be65d047578192975035d9bc4e786990a407a28a8f75b8/attacut-1.0.6-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 16.5MB/s \n",
            "\u001b[?25hCollecting ssg>=0.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/2b/9cf956a0088f44895d20ab0aa008d8f87cc1b1a210af14601aaf72dec729/ssg-0.0.6-py3-none-any.whl (473kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from attacut) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from attacut) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from attacut) (1.8.1+cu101)\n",
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from attacut) (0.6.2)\n",
            "Collecting nptyping>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/5b/e8c90a98b8462768ca43ad43021d404b81430fde28a6e8f93a8101fe9a8f/nptyping-1.4.1-py3-none-any.whl\n",
            "Collecting pyyaml>=5.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from ssg>=0.0.4->attacut) (0.9.7)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from ssg>=0.0.4->attacut) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->attacut) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->attacut) (1.1.0)\n",
            "Collecting typish>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/a7/83e450157d1613be0725821f8bd8aadab22217fa5dac4795dcfb9408be95/typish-1.9.2-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=12f126cb195813b391fd30d81e42208249d9f884954f9598d56d41a127ffc305\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, ssg, typish, nptyping, pyyaml, attacut\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed attacut-1.0.6 fire-0.4.0 nptyping-1.4.1 pyyaml-5.4.1 ssg-0.0.6 typish-1.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MuESi2FlpIa",
        "outputId": "2642369a-3bb5-4169-e9d1-66629862d68e"
      },
      "source": [
        "from pythainlp.tokenize import word_tokenize\n",
        "text = 'ณชพศ อินทรสมบัติ'\n",
        "word_tokenize(text, engine=\"newmm\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ณชพศ', ' ', 'อินทร', 'สมบัติ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u96hsQLfAhka"
      },
      "source": [
        "Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIc_A4dCiZyV"
      },
      "source": [
        "def featurize_one_sentence(tokens, feature_function_list):\n",
        "    feature_dict_seq = []\n",
        "    for i in range(len(tokens)): # ทุก token\n",
        "        feature_dict = {}\n",
        "        \n",
        "        for feature_fn in feature_function_list: # ทุก feature function\n",
        "\n",
        "            feature_dict.update(feature_fn(i, tokens))\n",
        "        feature_dict_seq.append(feature_dict)\n",
        "    # print(feature_dict_seq)\n",
        "    return feature_dict_seq\n",
        "    \n",
        "def train_and_evaluate(X_train, Y_train, X_test, feature_function_list):\n",
        "    token_seq_list = X_train\n",
        "    feature_seq_list = [featurize_one_sentence(token_list,feature_function_list) for token_list in token_seq_list]\n",
        "    label_seq_list=Y_train\n",
        "    #TODO: Fill this in\n",
        "    crf = sklearn_crfsuite.CRF(verbose=True)\n",
        "    crf.fit(feature_seq_list, label_seq_list)\n",
        "    pred = crf.predict(X_test)\n",
        "    \n",
        "    # print(sklearn_crfsuite.metrics.flat_classification_report(label_seq_list, pred))\n",
        "    \n",
        "    return pred\n",
        "\n",
        "\n",
        "def train_and_evaluate_butnofea(X_train, Y_train, X_test, Y_test):\n",
        "    crf = sklearn_crfsuite.CRF(verbose=True)\n",
        "    crf.fit(X_train, Y_train)\n",
        "    pred = crf.predict(X_test)\n",
        "    # print(sklearn_crfsuite.metrics.flat_classification_report(Y_test, pred))\n",
        "    \n",
        "    return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqvDuRURC0Cr"
      },
      "source": [
        "ห้องทดลอง"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX9C8xhci6-7",
        "outputId": "481a32e4-6f90-45d9-a2b2-5f4f6b29d7e1"
      },
      "source": [
        "X_train[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ysTK_8gfuh"
      },
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_json ('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/test_entities.json')\n",
        "df1.to_csv ('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/New File Name.txt', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bao7G7KcfSDt"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json ('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/train_entities.json')\n",
        "df.to_csv ('/content/drive/MyDrive/CONTEST 3/Contest 3 - NER/New File Name.txt', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk7BEkDtNANl"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mloz7T668SZl",
        "outputId": "99414319-221a-4a2c-e1dd-c3b94e850ce3"
      },
      "source": [
        "len(X_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68930"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-dT9tPx8UZR"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgE2jwuAtVqK"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF()\n",
        "# crf.fit(X_train, Y_train)\n",
        "crf.fit(X_all, Y_all)\n",
        "pred = crf.predict(list_test)\n",
        "# print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPsCUd1oJYQ7"
      },
      "source": [
        "feature_seq_list = [featurize_one_sentence(token_list,[pos_features,pos_conjunctive_features,word_features]) for token_list in X_train]\n",
        "crf = sklearn_crfsuite.CRF()\n",
        "\n",
        "new = []\n",
        "for len_sentence in range(len(X_train)):\n",
        "  _temp6 = []\n",
        "  for len_word in range(len(X_train[len_sentence])):\n",
        "    l = list(X_train[len_sentence][len_word])\n",
        "    # {'w0 word': 'เวลา 13.00 น.'}\n",
        "    a = feature_seq_list[len_sentence][len_word]\n",
        "    _list1 = list(a.items())\n",
        "    \n",
        "    # print(_list1)\n",
        "    _x = []\n",
        "    for w in _list1:\n",
        "      for s in w:\n",
        "        _x.append(s)\n",
        "\n",
        "    z = l  + _x\n",
        "    _temp6.append(tuple(z))\n",
        "  new.append(_temp6)\n",
        "\n",
        "# print(new[-2])\n",
        "# print(len(new))\n",
        "crf.fit(new, Y_train)\n",
        "pred = crf.predict(X_test_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcycCKWPZXq3"
      },
      "source": [
        "new[-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ueh6j0JRKls"
      },
      "source": [
        "asasd = ('เวลา 13.00 น.', 'NOUN')\n",
        "\n",
        "\n",
        "l = list(asasd)\n",
        "l.append('x')\n",
        "\n",
        "tuple(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyd39YzXP5R5"
      },
      "source": [
        "X_train[-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2aT1SVhP31c"
      },
      "source": [
        "feature_seq_list[-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ns9SFqR0j8F"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhxz6UPA554"
      },
      "source": [
        "อันนี้เป็นการดึง test โดยจะดึงคำของ test มาแล้วใส่ pos ให้มัน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8L0Inen24FY"
      },
      "source": [
        "import json\n",
        "with open('/content/test_entities.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# print(data)\n",
        "\n",
        "list_test = []\n",
        "for len_sentence in range(len(data)):\n",
        "  _temp1 = []\n",
        "  for len_word in range(len(data[len_sentence])):\n",
        "    if len(data[len_sentence]) != 0:\n",
        "      _temp1.append(data[len_sentence][len_word][1])\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  list_test.append(_temp1)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm3uwXw7JIpE",
        "outputId": "f2928fb0-a5d1-40bf-c534-2342324ac274"
      },
      "source": [
        "len(list_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOptPJ_N5DdS"
      },
      "source": [
        "from pythainlp.tag import pos_tag_sents\n",
        "\n",
        "X_test_2 = pos_tag_sents(list_test , corpus='orchid_ud')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz0hrBEsUkWA"
      },
      "source": [
        "list_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zii7LsDlAwfR"
      },
      "source": [
        "อันนี้คือเวลาเรา predict แล้วจะเอาสิ่งที่เรา predict ได้มา link กับคำ match (หาที่อยู่)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0xA6Ts0rhbt"
      },
      "source": [
        "list_evaluate = []\n",
        "for len_sentence in range(len(list_test)):\n",
        "  temp = [] \n",
        "  for len_word in range(len(list_test[len_sentence])):\n",
        "    temp2=[]\n",
        "    temp2.append(pred[len_sentence][len_word])\n",
        "    # print(X_test_2[len_sentence][len_word])\n",
        "    temp2.append(list_test[len_sentence][len_word]) #กรณีไม่มีไรเลย\n",
        "    # temp2.append(X_test_2[len_sentence][len_word][0]) #กรณีมี pos\n",
        "    temp.append(tuple(temp2))\n",
        "  list_evaluate.append(temp)\n",
        "\n",
        "json.dump(list_evaluate, open('predicted_test_entities.json', encoding='utf8', mode='w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt-VU7737MeF"
      },
      "source": [
        "X_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQCgfUjs7RQ9"
      },
      "source": [
        "Y_all"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}